# 1.5.2 Minimizing the expected loss

* Objective : more complex than simply minimizing the number of misclassifications.

## Type of error

### consider medical diagnosis

Type 1 error : a patient with cancer is diagnosed as healthy
Type 2 error : a patient who does not have cancer is incorrectly diagnosed as having cancer

> Usually `Type 1 error` is more important than `Type 2 error`

## Loss function (a.k.a Cost function)

* Formalize such issues (type of error) through the *loss function*

* Goal : Minimize the total loss incurred

* Utility function : negative of the loss 

> suppose that, for a new value of **x**, the true class is $C_k$ and that we assign **x** to class $C_j$ (where $j$ may or may not be equal to $k$).

In so doing, we incur some level of loss that we denote by $L_{kj}$

* $L_{kj}$ : Loss when classifying the actual k class into j class

ex) example of loss matrix
![](https://www.dropbox.com/s/a01gk3rqtjbufba/loss%20matrix.PNG?dl=1)

* Optimal solution : Minimizes the loss function
> however *loss function* depends on the true class, which is unknown.

* For a given input vector **x** uncertainty in the true class is expressed through the joint probability distribution $p(x, C_k)$

> so we seek instead to minimize the **expected loss** 

$$
E(L) = \sum_{k}^{} \sum_{j}^{} \int_{R_j} L_{kj} p(x, C_k) dx
$$  
where $R_j$ is decision regions

* Goal : Choose the region $R_j$ in order to minimize the *expected loss* 

> For each **x** we should minimize $\sum_{k} L_{kj} p(x, C_k)$ 
> meaning : Expected loss when x is classified as $j$ class.

Using the product rule $p(x, C_k)=p(C_k \mid x)p(x)$
The decision rule that minimizes the *expected loss* is the one that assigns each new **x** to the class $j$ for which the quantity
$$
\sum_{k} L_{kj} p(C_k \mid x)
$$
is a minimum. (eliminate the common factor of $p(x)$)

# 1.5.3 The reject option

Cases when classification errors arise

1. Largest $p(C_k \mid x)$ is significantly less then 1.
2. $p(x, C_k)$ have comparable values

> Uncertain about true class 

## Rejection option

Ex)
* Little doubt as to the correct class : automatic system to classify 
* More ambiguous cases : Leaving a human expert to classify

> Rejecting inputs **x** when $p(C_k \mid x) < \theta$
> If $\theta = 1$ : all inputs are rejected
> If $\theta < 1/K$ : no inputs are rejected (K: number of classes)

![](https://www.dropbox.com/s/d2ko4agy9cwmwjr/f1_26.PNG?dl=1)



## 1.5.4 Inference and decision 

We have broken the classification problem down into two separate stages

1. Inference stage : training data to learn a model for $p(C_k \mid x)$
2. Decision stage : use posterior probabilities to make optimal class assignments.

*discriminant function* : Simply learn a function that maps inputs **x** directly into decisions to solve both problems together.

There are three distinct approaches to solving decision problems

### a. *Generative models*
First Solve inference problem 
*  $p(x\midC_k)$
* $p(C_k)$

> $p(C_k)$ : Often be estimated simply from the training data 

Then use Bayes's theorem in the form
$$
p(C_k\midx) = \frac{p(x\midC_k)p(C_k)}{p(x)} = \frac{p(x\midC_k)p(C_k)}{\sum_{k}p(x\midC_k)p(C_k)}
$$
or

Model the $p(x, C_k)$ directly and then normalize to obtain the posterior probabilities

>  Having fount the posterior probabilities, we use decision theory to determine class for each new input **x**
>  Approaches that model the distribution of inputs as well as outputs are known as *generative models*, because by sampling from them it is possible to generate synthetic data points in the input space. 

### b. *Discriminative models*
First solve the inference problem
* $p(C_k\midx)$

Approaches that model the posterior probabilities directly are called *discriminative models*

### c. *Discriminant function*
Find a function $f(x)$, called discriminant function, which maps each input **x** directly onto a class label

### Relative merits of three alternatives.

### a. *Generative models*

**Pros**
*  Allows the marginal density of data $p(x)$ : Useful for *outlier detection* or *novelty detection*

**Cons**
* The most demanding : finding distribution over **x**(usually high dimension) and $C_k$
* Need a large training set
* we only really need the $p(C_k\midx)$


### b. *Discriminative models*
 
 **Pros**
 * $p(C_k\midx)$ obtained directly

![](https://www.dropbox.com/s/2xhqbktmvr4m8pg/f1_27.PNG?dl=1)


### c. *Discriminant function*

 **Pros**
* Combining the inference and decision stages

**Cons**
* No longer access to the $p(C_k\midx)$


### Powerful reasons for waning to compute the $p(C_k\midx)$

* **Minimizing risk**

Consider *Loss matrix* are changed from time to time
If we know the $p(C_k\midx)$, we can trivially revise the minimum risk decision criterion by modifying $\sum_{k} L_{kj} p(C_k \mid x)$

* **Reject options**

* **Compensating for class priors**

Unbalanced problem : 
Only 1 in every 1000 examples corresponds to the presence of cancer.

Assigned every samples to the normal class :
99.9% accuracy.

A Balanced data set : 
Allow us to find a more accurate model. 
-> we then have to compensate for the effects of our modifications to the training data

From 
$$
p(C_k\midx) = \frac{p(x\midC_k)p(C_k)}{p(x)}
$$
we see that the $p(C_k\midx)$ are proportional to the $p(C_k)$

Using the above relationship between $p(C_k\midx)$ and $p(C_k)$, define new posterior probabilities $p_{new}(C_k \midx^*)$
$$
p_{new}(C_k \midx^*) \propto p(C_k) \frac{p^*(C_k\midx^*)}{p^*(C_k)}
$$
where
$$
p(C_k) : \text{Prior of original data set} \\
p^*(C_k) : \text{Prior of balanced data set} \\
p^*(C_k\midx^*) : \text{Posterior of balanced data set}
$$

* **Combining models**



## 1.5.5 Loss functions for regression

For complex applications,
we wish to break the problem into a number of smaller subproblems each of which can be tackled by a **separate module**.

As long as each of the two models fives posterior probabilities for the classes, we can **combine the outputs systematically**

Example .

Assume 
$x_I$ : X-ray image
$x_B$ : Blood data
$p(x_I, x_B) = p(x_I)p(x_B)$

so that
$$
p(x_I, x_B \mid C_k) = p(x_I \mid C_k) p(x_B \mid C_k)
$$
This is an example of *conditional independence* property

The posterior probability is then given by
$$
\begin{aligned}
p(C_k \mid x_I, x_B) &\propto p(x_I, x_B \mid C_k)p(C_k)\\
&\propto p(x_I \mid C_k)p(x_B \mid C_k)p(C_k) \\
&\propto \frac{p(C_k \mid x_I)p(C_k \mid x_B)}{p(C_k)}
\end{aligned}
$$

The particular conditional independence assumption is an example of the *naive Bayes model*

## 1.5.5 Loss functions for regression

We now turn to the case of regression problems.

The decision stage consists of choosing a specific estimate $y(x)$ of the value of $t$ for each input **x**.

Suppose that in doing so, we incur a loss $L(t, y(x))$.

The averae, or expected, loss is then given by
$$
E(L) = \int\int L(t, y(x)) p(x, t) dx dt
$$ 

### Method 1.

A common choice of loss function in regression problems is the squared loss given by $L(t, y(x))=\{ y(x) - t \}^2$.

In this case, the expected loss can be written
$$
E(L) = \int\int \{ y(x) - t \}^2 p(x, t) dx dt
$$ 

Our goal is to choose $y(x)$ so as to minimize $E(L)$. If we assume a completely flexible function $y(x)$, we can do this formally using the calculus of variations to give
$$
\frac{\delta E(L)}{\delta y(x)} = 2 \int \{ y(x) - t \} p(x, t)dt=0
$$

Solving for $y(x)$, and using the sum and product rules of probability, we obtain
$$
y(x) = \frac{\int tp(x,t) dt}{p(x)} = \int t p(t\midx) dt = E_t(t\midx)
$$
which is the conditional average of $t$ conditioned on $x$ and is known as the *regression function*.

![](https://www.dropbox.com/s/d0b6634vprj4l4y/f1_28.PNG?dl=1)

### Method 2.

Armed with the knowledge that the optimal solution is the conditional expectation, we can expand the square term as follows
$$
\begin{aligned}
\{ y(x) - t \}^2 &= \{ y(x) -E(t\midx)+-E(t\midx) - t \}^2 \\
&= \{  y(x) - E(t\midx) \}^2  + 2\{ y(x) -E(t\midx) \}\{ E(t\midx) -t\} +
\{ E(t\midx) -t \}^2
\end{aligned}
$$
where to keep the notation uncluttered, we use$E(t\midx)$ to denote $E_t(t\midx)$.
Substituting into the loss function and performing the integral over $t$, we obtain an expression for the loss function in the form
$$
E(L)=\int \{  y(x) - E(t\midx) \}^2 p(x) dx + \int p(x) \int \{ E(t\midx) -t \}^2 p(t\midx)dt dx
$$

The function $y(x)$ we seek to determine enters only in the first term, which will be minimized when $y(x)$ is equal to $E(t\midx)$.

The second term is the variance of the distribution of $t$, averaged over $x$. It represents the intrinsic variability of the target data and can be regarded as noise. Because it is independent of $y(x)$, it represents the irreducible minimum value of the loss function.

We can identify three distinct approaches to solving regression problems given, in order of decreasing complexity, by:

a. First solve the inference problem of determining the joint density $p(x,t)$. Then normalize to find the conditional density $p(t\midx)$, and finally marginalize to find the conditional mean, $E_t(t\midx)$.  

b. First solve the inference problem of determining the conditional density $p(t\midx)$, and then subsequently marginalize to find the conditional mean, $E_t(t\midx)$. 

c. Find a regression function $y(x)$ directly from the training data.

### *Minkowski loss*

The squared loss is not the only possible choice of loss function for regression.

Here we consider briefly one simple generalization of the squared loss, called the $Minkowski loss$, whose expectation is given by
$$
E(L_q) = \int \int \mid  y(x) - t \mid^q p(x,t) dx dt
$$

The minimum of $E(L_q)$
q=2 : The conditional mean
q=1 : The conditional median
q=0 : The conditional mode 

![](https://www.dropbox.com/s/kzkd6elp3wlg1xz/f1_29.PNG?dl=1)

## 1.6 Information Theory

We have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the susequent discussion in this book.

We begin by considering a discrete random variable **x** and we ask how much information is received when we observe a specific value for this variable.

The amount of information can ve viewed as the `degree of surprise` on learning the value of **x**.

* Rare event : more information
* Common event : less information

> Our measure of information content will therefore depend on the probability distribution $p(x)$

Consider quantity $h(x)$
* monotonic function of the probability $p(x)$
* $h(x)$ expresses the information

If two events $x$ and $y$ are unrelated, $p(x,y)=p(x)p(y)$ and $h(x, y)$ should be $h(x) + h(y)$.

From these two relationships, it is easily shown that $h(x)$ must be given by the logarithm of $p(x)$ and so we have
$$
h(x) = -log_2 p(x)
$$ 
where the negative sign ensures that information is positive or zero.
* Low probability events $x$ correspond to high information content.
* The choice of basis for the logarithm is arbitrary
* The convention prevalent in information theory : logarithms to the base of 2
* When using basis of 2, units of $h(x)$ are `bits`

Now suppose that a sender wishes to transmit the value of a random variable to a receiver.

`The average amount of information` that they transmit in the process is obtained by taking the expectation of $h(x)$ with repect to the distribution $p(x)$ and is given by
$$
H[x] = -\sum_{x} p(x) log_2 p(x)
$$
This is called the `entropy` of the random variable **x**. 

* $\underset{p\rightarrow 0}{lim}$ $pln(p)=0$ so we shall take $p(x)log(p(x))=0$ whenever we encounter a value for $x$ such that $p(x)=0$

We now show that these definitions indeed possess usefull properties.

Consider a random variable $x$ having 8 possible states, each of which is equally likely.
In order to communicate the value of $x$ to a receiver, we would need to transmit a message of length 3 bits.
Notice that the entropy of this variable is given by
$$
H[x] = -8 \times \frac{1}{8} log_2 \frac{1}{8} = 3 \text{ bits}
$$
Now consider an example
* variable having 8 states : *{a,b,c,d,e,f,g,h}*
* respective probabilites : $(\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{64},\frac{1}{64},\frac{1}{64},\frac{1}{64})$

The entropy in this case is given by
$$
H[x] = -\frac{1}{2}log_2\frac{1}{2}
-\frac{1}{4}log_2\frac{1}{4}
-\frac{1}{8}log_2\frac{1}{8}
-\frac{1}{16}log_2\frac{1}{16}
-\frac{4}{64}log_2\frac{1}{64} = 2\text{ bits}
$$
* The nonuniform distribution has a smaller entropy than the uniform one

We can take advantage of the nonuniform distribution by using shorter codes for the more probable events.
* *{a,b,c,d,e,f,g,h}* coded *{0, 10, 110, 1110, 111100, 111101, 111110, 111111}*
* average code length = $\frac{1}{2}\times 1+\frac{1}{4}\times 2+\frac{1}{8}\times 3+\frac{1}{16}\times 4+4 \times \frac{1}{64}\times 6=2\text{ bits}$

which again is the same as the entropy of the random variable.
* The entropy is a lower bound on the number of bits needed to transmit the state of a random variable.

From now on, we shall switch to the use of natural logarithms in defining entropy
In this case, the entropy is measured in units of `nats` instead of bits, which differ simply by a factor of $ln2$

In fact, the concepts of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretaion as a measure of disorder through developments in statistical mechanics.

We can understand this alternative view of entropy by considering a set of $N$ identical objects that are to be divided amongst a set of bins, such that there are $n_i$ objects in the $i^{th}$ bin.

The total number of ways of allocating the $N$ objects to the bins is given by
* Arrange N objects in order. 
* Assigned to bins in order. 
* Order does not matter in the same bin.
$$
W=\frac{N!}{\prod_{i} n_i ! }
$$
which is called the *multiplicity*.

The entropy is then defined as the logarithm of the multiplicity scaled by an appropriate constant
$$
H=\frac{1}{N}ln W = \frac{1}{N}ln N! - \frac{1}{N}\sum_{i} ln(n_i !)
$$
We now consider the limit $N\rightarrow \infty$, in which the fractions $n_i/N$ are held fixed, and apply *Stirling's* approximation
$$
ln N! \simeq N ln N - N
$$
which gives
$$
H = \underset{N\rightarrow \infty}{lim} \sum_{i} (\frac{n_i}{N}) ln (\frac{n_i}{N}) = -\sum_{i}p_i ln( p_i)
$$
where we have used $\sum_{i} n_i=N$. Here $p_i = \underset{N\rightarrow \infty}{lim}(n_i/N)$ is the probability of an object being assigned to the $i^{th}$ bin.

We can interpret the bins as the states $x_i$ of a discrete random variable $X$, where $p(X=x_i)=p_i$. The entropy of the random variable $X$ is then
$$
H[p] = -\sum_{i} p(x_i) ln p(x_i).
$$
Distribution $p(x_i)$ that are sharply peaked around a few values will have a relatively low entropy, whereas those that are spread moe evenly across many values will have higher entropy.

![](https://www.dropbox.com/s/oj8ppsdw13ioj0d/f1_30.PNG?dl=1)

Because $0 \leq p_i \leq 1$, the entropy is nonnegative, and it will equal its minimum value of 0 when one of the $p_i=1$ and all other $p_{i\neq j}=0$. 

The maximum entropy configuration can be found by maximizing $H$ using a Lagrange multiplier to enforce the normalization constraint on the probabilities. Thus we maximize
$$
\tilde{H} = - \sum_{i} p(x_i) ln p(x_i) + \lambda ( \sum_{i} p(x_i) - 1 )
$$
from which we find that all of the $p(x_i)$ are equal and are given by $p(x_i)=1/M$ where $M$ is the total number of states $x_i$. The corresponding value of the entropy is then $H=ln M$.

### Continuous variable x

We can now quantize the contibuous variable $x$ by assigning any value $x$ to the value $x_i$ whenever $x$ falls in the $i^{th}$ bin.

First divide $x$ into bins of width $\Delta$.
Then, assuming $p(x)$ is continuous, there must exist a value $x_i$ such that
$$
\int_{i\Delta}^{(i+1)\Delta} p(x) dx = p(x_i) \Delta.
$$
The probability of observing the value $x_i$ is then $p(x_i)\Delta$. This gives a discrete distribution for which the entropy takes the form
$$
H_\Delta = -\sum_{i} p(x_i)\Delta ln(p(x_i)\Delta) = -\sum_{i} p(x_i)\Delta lnp(x_i) - ln \Delta
$$
where we have used $\sum_{i} p(x_i)\Delta = 1$.

We now omit the second term $- ln \Delta$ and then consider the limit $\Delta \rightarrow 0$.

The first term on the right-hand side will approach the integral of $p(x)ln(p(x))$ in this limit so that
$$
\underset{\Delta\rightarrow 0}{lim} \{  \sum_{i} p(x_i)\Delta ln( p(x_i) ) \} = -\int p(x) ln(p(x)) dx
$$
where the quantity on the right-hand side is called the *differential entropy*. 

* The discrete and continuous forms of the entropy differ by a quantity $ln\Delta$, which diverges in the limit $\Delta \rightarrow 0$
* This reflects the fact that to specify a continuous variable very precisely requires a large number of bits

Let us now consider the maximum entropy configuration for a continuous variable.
We maximize the differential entropy with the three constaints
$$
\begin{aligned}
\int_{-\infty}^{\infty} p(x) dx &= 1 \\
\int_{-\infty}^{\infty} x p(x) dx &= \mu \\
\int_{-\infty}^{\infty} (x-\mu)^2 p(x) dx &= \sigma^2
\end{aligned}
$$
The constrained maximization can be performed using Lagrange multipliers so that we maximize the following functional with respect to $p(x)$
$$
-\int_{-\infty}^{\infty} p(x) ln( p(x) ) dx + \lambda_1(\int_{-\infty}^{\infty} p(x) dx - 1) \\
+\lambda_2(\int_{-\infty}^{\infty} x p(x) dx - \mu) + \lambda_3(\int_{-\infty}^{\infty} (x-\mu)^2 p(x) dx - \sigma^2)
$$
Finally the result is given by
$$
p(x) = \frac{1}{(2\pi \sigma^2)^{1/2}} exp\{  -\frac{(x-\mu)^2}{2\sigma^2} \}
$$
So the distribution that maximizes the differential entropy is the **Gaussian**.

If we evaluate the differential entropy of the Gaussian, we obtain
$$
H[x] = \frac{1}{2} \{  1 + ln( 2 \pi \sigma^2 )  \}
$$
* The entropy increases as the distribution becomes broader, i.e., as $\sigma^2$ increases

Suppose we have a joint distribution $p(x,y)$ from which we draw pairs of values of $x$ and $y$.
If a value of $x$ is already known, then the additional information needed to specify the corresponding value of $y$ is given by $-ln(p(y_x))$. 
Thus the average additional information needed to specify $y$ can be written as
$$
H[y\midx] = -\int \int p(y,x) ln( p(y\midx) ) dy dx
$$
which is called the *conditional entropy* of $y$ given $x$.

It is easily seen, using the product rule, that the conditional entropy satisfies the relation
$$
H[x,y] = H[y\midx] + H[x]
$$
Thus the information needed to describe $x$ and $y$ is given by the sum of the information needed to describe $x$ alone plus the addional information required to specify $y$ given $x$


## 1.6.1 Relative entropy and mutual information

Consider some unknown distribution $p(x)$, and suppose that we have modelled this using an approximating distribution $q(x)$

The average *additional* amount of information (in nats) required to specify the value of $x$ as a result of using $q(x)$ instead of the true distribution $p(x)$ is given by
$$
\begin{aligned}
KL(p\mid\midq) &= -\int p(x) ln q(x) dx - (  -\int p(x) ln p(x) dx   ) \\
&= -\int p(x) ln \frac{q(x)}{p(x)} dx 
\end{aligned}
$$
This is known as the *relative entropy* or *Kullback-Leibler divergence*, or KL *divergence*, between the distribution $p(x)$ and $q(x)$. 

* Note that $KL(p\mid\midq)\neq KL(q\mid\midp)$

### Show that $KL(p\mid\midq) \geq 0$

For continuous variables, Jensen's inequality takes the form
$$
f(  \int xp(x) dx ) \leq \int f(x) p(x) dx
$$ 
We can apply Jensen's inequality to the Kullback-Leibler divergence to give
$$
KL(p\mid\midq) = -\int p(x) ln \frac{q(x)}{p(x)} dx \geq -ln \int q(x) dx = 0
$$
where we have used the fact that $-ln x$ is convex function,  together with the normalization condition $\int q(x) dx = 1$.
In fact, $-ln x$ is a strictly convex function, so the equality will hold if, and only if, $q(x)=p(x)$ for all $x$.

* Thus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions $p(x)$ and $q(x)$
* The additional information that must be transmitted is (at least) equal to the Kullback-Leibler divergence between the two distributions.

Suppose the data is being generated from an unknown distribution $p(x)$

* Approximate $p(x)$ using $q(x\mid\theta)$ governed by adjustable parameters $\theta$

Find $\theta$ to minimize the Kullback-Leibler divergence
$$
\underset{\theta}{argmin}  KL(  p(x) \mid q(x\mid\theta) )
$$
However, we cannot do this directly because we don't know $p(x)$.

Suppose that we have observed a finite set of training points $x_n$, for $n=1,...,N$, drawn from $p(x)$.

Then the expectation with respect to $p(x)$ can be approximated by a finite sum over these points, using Law of Large number. so that
$$
KL(p\mid\midq)  \simeq  \sum_{n=1}^{N} \{  -ln q(x_n \mid\theta)  + ln p(x_n) \}
$$
The second term is independent of $\theta$, and the first term is the negative log likelihood function for $\theta$ under the distribution $q(x\mid\theta)$ evaluated using the training set.

* Thus we see that minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function since $KL(p\mid\midq) \geq 0$

We can gain some idea of whether two sets of variables $x$ and $y$ are close to being independent by considering the Kullback-Leibler divergence between the joint distribution and the product of the marginals, given by
$$
\begin{aligned}
I[x,y] &\equiv KL(   p(x,y)  \mid p(x)p(y) ) \\
&= -\int \int p(x,y) ln(  \frac{p(x)p(y)}{p(x,y)}   )  dx dy
\end{aligned}
$$
which is called the *mutual information* between the variables $x$ and $y$. 

From the properties of the Kullback-Leibler divergence, we see that $I(x,y) \geq 0$.

We see that the mutual information is related to the conditional entropy through
$$
I(x,y)=H(x) - H(x\midy) = H(y) - H(y\midx).
$$
From a Bayesian perspective, we can view $p(x)$ as the prior distribution for $x$ and $p(x\midy)$ as posterior distribution after we haver observed new data $y$.

The mutual information therefore represents the reduction in uncertainty about $x$ as a consequence of the new observation $y$.

![](https://www.dropbox.com/s/m2x16hl3yt8xhyk/mutual_info.png?dl=1)





